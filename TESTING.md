# üß™ Comprehensive Testing Guide

This document provides detailed information about the testing infrastructure for the CustomLangGraphChatBot project.

## üìã Table of Contents

1. [Test Structure](#test-structure)
2. [Test Categories](#test-categories)
3. [Running Tests](#running-tests)
4. [Test Coverage](#test-coverage)
5. [Writing Tests](#writing-tests)
6. [CI/CD Integration](#cicd-integration)
7. [Troubleshooting](#troubleshooting)

## üèóÔ∏è Test Structure

The project includes comprehensive test coverage with 10+ test files:

```
tests/
‚îú‚îÄ‚îÄ conftest.py                    # Shared pytest fixtures and configuration
‚îú‚îÄ‚îÄ test_ai_analysis_tools.py      # AI tools with Grok API mocking (300 lines)
‚îú‚îÄ‚îÄ test_static_analysis_tools.py  # Static analysis tools testing (300 lines)
‚îú‚îÄ‚îÄ test_github_tools.py           # GitHub API integration tests (300 lines)
‚îú‚îÄ‚îÄ test_filesystem_tools.py       # File system operation tests (300 lines)
‚îú‚îÄ‚îÄ test_communication_tools.py    # Communication tools testing (300 lines)
‚îú‚îÄ‚îÄ test_registry.py               # Tool registry and configuration (300 lines)
‚îú‚îÄ‚îÄ test_workflow_integration.py   # End-to-end workflow tests (300 lines)
‚îú‚îÄ‚îÄ test_performance.py            # Performance and load tests (300 lines)
‚îú‚îÄ‚îÄ test_error_handling.py         # Error scenarios and edge cases (300 lines)
‚îî‚îÄ‚îÄ test_api_integration.py        # FastAPI endpoint tests (300 lines)
```

**Total Test Coverage**: 3000+ lines of test code covering all components.

## üéØ Test Categories

### 1. **Unit Tests** (300+ test cases)
- **Purpose**: Test individual tool functionality in isolation
- **Coverage**: All 25+ external tools, registry, configuration
- **Mocking**: External APIs, subprocess calls, file system operations
- **Files**: `test_*_tools.py`, `test_registry.py`

### 2. **Integration Tests** (50+ test cases)
- **Purpose**: Test end-to-end workflow execution
- **Coverage**: State management, node interactions, LangGraph workflows
- **Files**: `test_workflow_integration.py`

### 3. **Performance Tests** (25+ test cases)
- **Purpose**: Benchmark tool execution and system performance
- **Coverage**: Execution timing, memory usage, concurrent operations
- **Files**: `test_performance.py`

### 4. **Error Handling Tests** (100+ test cases)
- **Purpose**: Test error scenarios and edge cases
- **Coverage**: Network failures, invalid inputs, file system errors
- **Files**: `test_error_handling.py`

### 5. **API Tests** (40+ test cases)
- **Purpose**: Test FastAPI endpoints and web interface
- **Coverage**: Request/response validation, authentication, CORS
- **Files**: `test_api_integration.py`

## üöÄ Running Tests

### Quick Start

```bash
# Run all tests with coverage
python test_runner.py

# Run specific category
python test_runner.py ai           # AI analysis tools
python test_runner.py analysis     # Static analysis tools
python test_runner.py github       # GitHub integration tools
python test_runner.py filesystem   # File system tools
python test_runner.py communication # Communication tools
```

### Pytest Commands

```bash
# Run all tests
pytest tests/ -v

# Run specific test files
pytest tests/test_ai_analysis_tools.py -v
pytest tests/test_workflow_integration.py -v
pytest tests/test_performance.py -v

# Run tests by category markers
pytest tests/ -m "unit" -v          # Unit tests only
pytest tests/ -m "integration" -v   # Integration tests only
pytest tests/ -m "performance" -v   # Performance tests only
pytest tests/ -m "api" -v           # API tests only

# Run tests with coverage
pytest --cov=src --cov=tools tests/ --cov-report=html --cov-report=term

# Run tests in parallel (faster execution)
pytest tests/ -n auto  # Requires: pip install pytest-xdist

# Run with detailed output
pytest tests/ -v -s --tb=long

# Run performance tests with timing
pytest tests/test_performance.py -v -s --durations=10
```

### Advanced Test Runner Options

```bash
# Enhanced test runner with options
python test_runner.py --help

# Run only pytest tests
python test_runner.py --pytest-only

# Run only manual integration tests
python test_runner.py --manual-only

# Run performance benchmarks
python test_runner.py --performance

# Generate coverage report
python test_runner.py --coverage

# Quick test run (no coverage)
python test_runner.py --quick

# Verbose output
python test_runner.py --verbose
```

## üìä Test Coverage

### Coverage Metrics
- **Overall Coverage**: >90%
- **Critical Components**: >95%
- **Tool Coverage**: 100% (all 25+ tools tested)
- **API Coverage**: >95%
- **Error Handling**: >90%
- **Integration Paths**: >85%

### Coverage Reports

```bash
# Generate HTML coverage report
pytest --cov=src --cov=tools tests/ --cov-report=html
# Open htmlcov/index.html in browser

# Generate XML coverage report (for CI/CD)
pytest --cov=src --cov=tools tests/ --cov-report=xml

# Terminal coverage report
pytest --cov=src --cov=tools tests/ --cov-report=term-missing
```

### Coverage Configuration

The project uses `pytest.ini` for consistent coverage configuration:

```ini
[tool:pytest]
testpaths = tests
addopts = 
    --cov=src
    --cov=tools
    --cov-report=term
    --cov-report=html:htmlcov
    --cov-fail-under=90
```

## ‚úçÔ∏è Writing Tests

### Test Structure Guidelines

1. **Use pytest fixtures** from `conftest.py`
2. **Mock external dependencies** (APIs, subprocess, file system)
3. **Test both success and failure scenarios**
4. **Include edge cases and error conditions**
5. **Use descriptive test names and docstrings**

### Example Test Structure

```python
"""Test module for ExampleTool."""

import pytest
from unittest.mock import Mock, patch
from tools.example_tool import ExampleTool

class TestExampleTool:
    """Test ExampleTool functionality."""
    
    def setup_method(self):
        """Set up test fixtures."""
        self.tool = ExampleTool()
    
    def test_successful_execution(self):
        """Test successful tool execution."""
        result = self.tool._run("valid_input")
        assert "error" not in result
        assert result["status"] == "success"
    
    @patch('external_dependency.api_call')
    def test_with_mocked_dependency(self, mock_api):
        """Test with mocked external dependency."""
        mock_api.return_value = {"data": "test"}
        result = self.tool._run("input")
        assert result["data"] == "test"
    
    def test_error_handling(self):
        """Test error handling."""
        result = self.tool._run("invalid_input")
        assert "error" in result
        assert "invalid" in result["error"].lower()
```

### Fixtures Usage

```python
def test_with_temp_directory(temp_directory):
    """Test using temporary directory fixture."""
    # temp_directory is automatically created and cleaned up
    
def test_with_sample_repository(sample_repository):
    """Test using sample repository fixture."""
    # sample_repository contains a complete test repository structure
    
def test_with_tool_registry(tool_registry):
    """Test using tool registry fixture."""
    # tool_registry is pre-configured for testing
```

## üîÑ CI/CD Integration

### GitHub Actions Workflow

The project includes a comprehensive GitHub Actions workflow (`.github/workflows/tests.yml`):

- **Multi-Python Version Testing**: Python 3.9, 3.10, 3.11, 3.12
- **Test Categories**: Unit, Integration, Performance, API
- **Code Quality**: Linting, formatting, type checking
- **Security**: Bandit security analysis, dependency checking
- **Coverage**: Automated coverage reporting with Codecov

### Workflow Triggers

- **Push**: to main/develop branches
- **Pull Request**: to main/develop branches  
- **Schedule**: Daily at 2 AM UTC
- **Manual**: Can be triggered manually

### Local CI Simulation

```bash
# Run the same tests as CI
python test_runner.py --pytest-only

# Run linting checks
black --check src/ tools/ tests/
flake8 src/ tools/ tests/
pylint src/ tools/

# Run security checks
bandit -r src/ tools/
safety check
```

## üîß Troubleshooting

### Common Issues

#### 1. **Import Errors**
```bash
# Ensure project root is in Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

#### 2. **Missing Dependencies**
```bash
# Install test dependencies
pip install pytest pytest-cov pytest-xdist pytest-mock
pip install pylint flake8 bandit
```

#### 3. **API Key Issues**
```bash
# Set up test environment
cp .env.example .env
# Edit .env with your API keys
```

#### 4. **Permission Errors**
```bash
# Fix file permissions
chmod +x test_runner.py
chmod +x validate_setup.py
```

### Debug Mode

```bash
# Run tests with debug output
pytest tests/ -v -s --tb=long --capture=no

# Run specific test with debugging
pytest tests/test_ai_analysis_tools.py::TestCodeReviewTool::test_successful_review -v -s
```

### Performance Issues

```bash
# Run tests in parallel
pytest tests/ -n auto

# Skip slow tests
pytest tests/ -m "not slow"

# Profile test execution
pytest tests/ --durations=10
```

## üìà Continuous Improvement

### Adding New Tests

1. **Create test file** following naming convention `test_*.py`
2. **Add appropriate markers** for categorization
3. **Include in CI workflow** if needed
4. **Update documentation** with new test information

### Test Metrics Monitoring

- **Coverage trends**: Monitor coverage over time
- **Performance benchmarks**: Track execution time changes
- **Failure rates**: Monitor test stability
- **Code quality**: Track linting and security metrics

---

For more information, see the main [README.md](README.md) or run `python test_runner.py --help`.
