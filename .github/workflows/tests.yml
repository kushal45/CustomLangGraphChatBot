name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  # Test environment variables for GitHub Actions
  GITHUB_VERIFY_SSL: "false"
  XAI_API_KEY: "test-xai-key"
  GROQ_API_KEY: "test-groq-key"
  SLACK_WEBHOOK_URL: "https://hooks.slack.com/test"
  EMAIL_SMTP_SERVER: "smtp.test.com"
  EMAIL_USERNAME: "test@example.com"
  EMAIL_PASSWORD: "test-password"
  JIRA_URL: "https://test.atlassian.net"
  JIRA_USERNAME: "test@example.com"
  JIRA_API_TOKEN: "test-jira-token"

jobs:
  # Fast-fail job to catch obvious issues quickly
  quick-check:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install minimal dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest

    - name: Quick syntax and import check
      run: |
        python -m py_compile tools/*.py
        python -c "from tools.registry import ToolRegistry; print('✅ Registry imports OK')"
        python -c "from api import app; print('✅ API imports OK')"
        python -c "from workflow import create_review_workflow; print('✅ Workflow imports OK')"

    - name: Quick test run (fastest tests only)
      run: |
        python -m pytest tests/unit/test_registry.py -v --tb=short --disable-warnings

  test:
    needs: quick-check
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Overall job timeout to prevent hanging
    strategy:
      fail-fast: false  # Continue running other jobs even if one fails
      matrix:
        python-version: ["3.12"]
        test-category: [unit, integration, performance]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/requirements-test.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y git

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Display environment information
      run: |
        echo "🔍 Environment Information:"
        echo "=========================="
        echo "Python version: $(python --version)"
        echo "Pip version: $(pip --version)"
        echo "Test category: ${{ matrix.test-category }}"
        echo "Runner OS: ${{ runner.os }}"
        echo ""
        echo "📦 Key package versions:"
        pip show pytest pytest-cov pytest-xdist pytest-benchmark || echo "Some packages not found"
        echo ""
        echo "💾 Available disk space:"
        df -h
        echo ""
        echo "📁 Current directory contents:"
        ls -la
        # Verify pytest-timeout is available
        python -c "import pytest_timeout; print('pytest-timeout installed successfully')"

    - name: Set up test environment
      run: |
        # Set GitHub token for real API tests (if available)
        if [ -n "${{ secrets.GITHUB_TOKEN }}" ]; then
          echo "GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}" >> $GITHUB_ENV
        else
          echo "GITHUB_TOKEN=test-github-token" >> $GITHUB_ENV
        fi
    
    - name: Run unit tests (all individual component tests)
      if: matrix.test-category == 'unit'
      timeout-minutes: 25
      run: |
        echo "🧪 Starting unit tests..."
        echo "Test files to be executed:"
        find tests/unit/ -name "*.py" -type f | head -10
        echo ""

        python -m pytest \
          tests/unit/ \
          -m "not integration and not real_params and not performance" \
          --tb=long \
          -v \
          --disable-warnings \
          --cov=tools \
          --cov=scripts \
          --cov=nodes \
          --cov-report=xml \
          --cov-report=term-missing \
          --maxfail=15 \
          -n 2 \
          --durations=10 \
          --junit-xml=unit-results.xml

    - name: Run integration tests (component interaction and workflow tests)
      if: matrix.test-category == 'integration'
      timeout-minutes: 30
      run: |
        echo "🔄 Starting integration tests..."
        echo "Test files to be executed:"
        find tests/integration/ -name "*.py" -type f
        echo ""

        python -m pytest \
          tests/integration/ \
          -m "not real_params and not performance" \
          --tb=long \
          -v \
          --disable-warnings \
          --cov=tools \
          --cov=workflow \
          --cov=nodes \
          --cov=api \
          --cov-report=xml \
          --cov-report=term-missing \
          --maxfail=10 \
          -n 2 \
          --durations=15 \
          --junit-xml=integration-results.xml

    - name: Run performance tests (load and stress testing)
      if: matrix.test-category == 'performance'
      timeout-minutes: 15
      run: |
        echo "🚀 Starting performance tests..."
        echo "Test files to be executed:"
        find tests/performance/ -name "*.py" -type f
        echo ""
        echo "📊 Performance test details:"
        python -m pytest tests/performance/ --collect-only -q
        echo ""

        # Run with detailed output and error handling
        set +e  # Don't exit immediately on error
        python -m pytest \
          tests/performance/ \
          --tb=long \
          -v \
          -s \
          --capture=no \
          --durations=20 \
          --maxfail=5 \
          --benchmark-skip \
          --junit-xml=performance-results.xml \
          --html=performance-report.html \
          --self-contained-html \
          --log-cli-level=INFO \
          --log-cli-format='%(asctime)s [%(levelname)8s] %(name)s: %(message)s' \
          --log-cli-date-format='%Y-%m-%d %H:%M:%S'

        PYTEST_EXIT_CODE=$?

        echo ""
        echo "📊 Performance test execution completed with exit code: $PYTEST_EXIT_CODE"

        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "❌ Performance tests failed"
          echo "📋 Showing detailed error information..."

          # Show any generated files
          echo "Generated files:"
          ls -la *.xml *.html 2>/dev/null || echo "No XML/HTML files generated"

          # Show JUnit XML if it exists
          if [ -f "performance-results.xml" ]; then
            echo ""
            echo "📄 JUnit XML results:"
            cat performance-results.xml
          fi

          # Show any pytest logs
          if [ -f "pytest.log" ]; then
            echo ""
            echo "📄 Pytest log:"
            cat pytest.log
          fi

          # Create a failure summary
          echo ""
          echo "🔍 Failure Summary:" > performance-failure.txt
          echo "Exit Code: $PYTEST_EXIT_CODE" >> performance-failure.txt
          echo "Timestamp: $(date)" >> performance-failure.txt
          echo "Test Category: performance" >> performance-failure.txt
          echo "Python Version: ${{ matrix.python-version }}" >> performance-failure.txt

          set -e  # Re-enable exit on error
          exit $PYTEST_EXIT_CODE
        else
          echo "✅ Performance tests passed successfully"
        fi

    - name: Capture performance test failure details
      if: failure() && matrix.test-category == 'performance'
      run: |
        echo "🔍 Capturing detailed performance test failure information..."

        # Create failure report directory
        mkdir -p failure-details

        # Capture current directory state
        echo "=== Directory Contents ===" > failure-details/directory-state.txt
        ls -la >> failure-details/directory-state.txt

        # Capture any generated test files
        echo "=== Generated Test Files ===" >> failure-details/directory-state.txt
        find . -name "*.xml" -o -name "*.html" -o -name "*.log" -o -name "*.txt" | head -20 >> failure-details/directory-state.txt

        # Capture pytest cache information
        if [ -d ".pytest_cache" ]; then
          echo "=== Pytest Cache Contents ===" > failure-details/pytest-cache.txt
          find .pytest_cache -type f | head -10 >> failure-details/pytest-cache.txt
        fi

        # Capture environment variables
        echo "=== Environment Variables ===" > failure-details/environment.txt
        env | grep -E "(PYTHON|PATH|PYTEST|TEST)" >> failure-details/environment.txt

        # Capture Python package information
        echo "=== Python Packages ===" > failure-details/packages.txt
        pip list >> failure-details/packages.txt

        # Try to run a simple performance test to see what's failing
        echo "=== Simple Test Execution ===" > failure-details/simple-test.txt
        python -c 'import sys\nprint(f"Python version: {sys.version}")\ntry:\n    import pytest\n    print(f"Pytest version: {pytest.__version__}")\nexcept Exception as e:\n    print(f"Pytest import error: {e}")\n\ntry:\n    import tests.performance\n    print("Performance tests module imported successfully")\nexcept Exception as e:\n    print(f"Performance tests import error: {e}")' >> failure-details/simple-test.txt 2>&1

        echo "📊 Failure details captured in failure-details/ directory"
        ls -la failure-details/

    - name: Collect test logs and artifacts
      if: always()  # Run even if tests fail
      run: |
        echo "📊 Collecting test artifacts and logs..."

        # Create artifacts directory
        mkdir -p test-artifacts

        # Always create a basic info file
        echo "Test Category: ${{ matrix.test-category }}" > test-artifacts/test-info.txt
        echo "Python Version: ${{ matrix.python-version }}" >> test-artifacts/test-info.txt
        echo "Runner OS: ${{ runner.os }}" >> test-artifacts/test-info.txt
        echo "Timestamp: $(date)" >> test-artifacts/test-info.txt
        echo "Workflow Run: ${{ github.run_id }}" >> test-artifacts/test-info.txt

        # Collect system information
        echo "=== System Information ===" > test-artifacts/system-info.txt
        echo "Python version: $(python --version)" >> test-artifacts/system-info.txt
        echo "Pip version: $(pip --version)" >> test-artifacts/system-info.txt
        echo "Current directory: $(pwd)" >> test-artifacts/system-info.txt
        echo "Available disk space:" >> test-artifacts/system-info.txt
        df -h >> test-artifacts/system-info.txt
        echo "" >> test-artifacts/system-info.txt
        echo "Directory contents:" >> test-artifacts/system-info.txt
        ls -la >> test-artifacts/system-info.txt

        # Collect pytest results
        if [ -f "unit-results.xml" ]; then
          cp unit-results.xml test-artifacts/
          echo "✅ Unit test results collected"
        else
          echo "❌ No unit test results found" >> test-artifacts/test-info.txt
        fi

        if [ -f "integration-results.xml" ]; then
          cp integration-results.xml test-artifacts/
          echo "✅ Integration test results collected"
        else
          echo "❌ No integration test results found" >> test-artifacts/test-info.txt
        fi

        if [ -f "performance-results.xml" ]; then
          cp performance-results.xml test-artifacts/
          echo "✅ Performance test results collected"
        else
          echo "❌ No performance test results found" >> test-artifacts/test-info.txt
        fi

        if [ -f "performance-report.html" ]; then
          cp performance-report.html test-artifacts/
          echo "✅ Performance HTML report collected"
        else
          echo "❌ No performance HTML report found" >> test-artifacts/test-info.txt
        fi

        # Collect coverage reports
        if [ -f "coverage.xml" ]; then
          cp coverage.xml test-artifacts/
          echo "✅ Coverage report collected"
        else
          echo "❌ No coverage report found" >> test-artifacts/test-info.txt
        fi

        # Collect any log files that might have been created
        if [ -d "logs" ]; then
          cp -r logs test-artifacts/ 2>/dev/null && echo "✅ Logs directory collected" || echo "❌ Failed to copy logs directory"
        else
          echo "❌ No logs directory found" >> test-artifacts/test-info.txt
        fi

        # Collect any pytest cache or temporary files
        if [ -d ".pytest_cache" ]; then
          cp -r .pytest_cache test-artifacts/ 2>/dev/null && echo "✅ Pytest cache collected" || echo "❌ Failed to copy pytest cache"
        fi

        # Collect failure details if they exist
        if [ -d "failure-details" ]; then
          cp -r failure-details test-artifacts/ 2>/dev/null && echo "✅ Failure details collected" || echo "❌ Failed to copy failure details"
        fi

        # Collect any additional failure files
        if [ -f "performance-failure.txt" ]; then
          cp performance-failure.txt test-artifacts/ && echo "✅ Performance failure summary collected"
        fi

        # Show test summary
        echo ""
        echo "📋 Test Artifacts Summary:"
        ls -la test-artifacts/
        echo ""
        echo "📄 Test Info Contents:"
        cat test-artifacts/test-info.txt

    - name: Upload test artifacts
      if: always()  # Upload even if tests fail
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.test-category }}-${{ matrix.python-version }}-${{ github.run_id }}
        path: test-artifacts/
        retention-days: 30
        if-no-files-found: warn  # Don't fail if no files found, just warn

    - name: Display test failure details
      if: failure()
      run: |
        echo "❌ Test execution failed for category: ${{ matrix.test-category }}"
        echo ""
        echo "📊 System Information:"
        echo "Python version: ${{ matrix.python-version }}"
        echo "Test category: ${{ matrix.test-category }}"
        echo "Runner OS: ${{ runner.os }}"
        echo ""
        echo "📋 Available files in workspace:"
        ls -la
        echo ""
        echo "🔍 Checking for error logs..."
        if [ -f "pytest.log" ]; then
          echo "Pytest log found:"
          cat pytest.log
        fi

        echo ""
        echo "💾 Disk space:"
        df -h
        echo ""
        echo "🧠 Memory usage:"
        free -h || echo "Memory info not available"

    - name: Test debugging scripts functionality
      if: matrix.test-category == 'unit'
      timeout-minutes: 15
      run: |
        echo "🔧 Testing debugging scripts functionality..."

        # Test node debugging script
        echo "Testing node debugging script..."
        python scripts/debug_node.py --node start_review_node --sample-state --output-file test_state.json || {
          echo "❌ Node debugging script failed"
          exit 1
        }
        echo "✅ Node debugging script passed"

        # Test state inspection script
        echo "Testing state inspection script..."
        python scripts/inspect_state.py --file test_state.json --analyze || {
          echo "❌ State inspection script failed"
          exit 1
        }
        echo "✅ State inspection script passed"

        # Test node serialization system
        echo "Testing node serialization system..."
        python scripts/node_serialization.py || {
          echo "❌ Node serialization script failed"
          exit 1
        }
        echo "✅ Node serialization script passed"

        # Test node replay system
        echo "Testing node replay system..."
        python scripts/node_replay.py || {
          echo "❌ Node replay script failed"
          exit 1
        }
        echo "✅ Node replay script passed"

        # Test node profiling system
        echo "Testing node profiling system..."
        python scripts/node_profiling.py || {
          echo "❌ Node profiling script failed"
          exit 1
        }
        echo "✅ Node profiling script passed"

        # Test flow visualization system
        echo "Testing flow visualization system..."
        python scripts/node_flow_diagrams.py || {
          echo "❌ Flow visualization script failed"
          exit 1
        }
        echo "✅ Flow visualization script passed"

        echo "🎉 All debugging scripts executed successfully"

    - name: Test workflow debugging capabilities
      if: matrix.test-category == 'integration'
      timeout-minutes: 10
      run: |
        # Test workflow state transitions
        python -c "
        import asyncio
        from tests.integration.test_workflow_debugging import WorkflowTestFixtures
        from nodes import start_review_node, analyze_code_node, generate_report_node

        async def test_workflow():
            state = WorkflowTestFixtures.create_initial_state()
            result1 = await start_review_node(state)
            state.update(result1)
            result2 = await analyze_code_node(state)
            state.update(result2)
            result3 = await generate_report_node(state)
            state.update(result3)
            print('✅ Workflow execution test passed')

        asyncio.run(test_workflow())
        "

        echo "✅ Workflow debugging capabilities verified"

    - name: Upload coverage to Codecov
      if: matrix.test-category == 'unit' || matrix.test-category == 'integration'
      continue-on-error: true
      timeout-minutes: 3
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: ${{ matrix.test-category }}
        name: codecov-${{ matrix.python-version }}-${{ matrix.test-category }}
        fail_ci_if_error: false

  # Sanity check job for critical functionality
  sanity-check:
    runs-on: ubuntu-latest
    needs: quick-check
    strategy:
      matrix:
        sanity-test: [
          "test_filesystem_tools_sanity",
          "test_analysis_tools_sanity",
          "test_ai_analysis_tools_sanity",
          "test_github_tools_sanity",
          "test_communication_tools_sanity",
          "test_debugging_tools_sanity",
          "test_workflow_integration_sanity"
        ]
      fail-fast: false  # Continue running other tests even if one fails

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run sanity check - ${{ matrix.sanity-test }}
      timeout-minutes: 15
      run: |
        python -m pytest tests/unit/test_module_sanity.py::${{ matrix.sanity-test }} \
          --tb=short -v --disable-warnings -s
      continue-on-error: true  # Allow individual sanity tests to fail without stopping the workflow

  lint:
    runs-on: ubuntu-latest
    needs: quick-check
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run black (code formatting)
      run: |
        black --check --diff tools/ tests/ || true

    - name: Run isort (import sorting)
      run: |
        isort --check-only --diff tools/ tests/ || true

    - name: Run flake8 (style guide)
      run: |
        flake8 tools/ tests/ --max-line-length=100 --extend-ignore=E203,W503,E501 || true

    - name: Run pylint (static analysis)
      run: |
        pylint tools/ --disable=C0114,C0115,C0116,R0903,R0913,W0613 --max-line-length=100 || true

  security:
    runs-on: ubuntu-latest
    needs: quick-check
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run bandit (security analysis)
      run: |
        bandit -r tools/ -f json -o bandit-report.json || true
        bandit -r tools/ || true

    - name: Run safety (dependency security)
      run: |
        safety check --json --output safety-report.json || true
        safety check || true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  integration-test:
    runs-on: ubuntu-latest
    needs: [test, lint, sanity-check]
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run segregated test suite
      run: |
        python -m pytest \
          -m "not integration and not real_params and not performance" \
          --tb=short \
          -v \
          --disable-warnings \
          -n auto

    - name: Validate setup
      run: |
        python validate_setup.py

    - name: Test tool registry
      run: |
        python -c "
        from tools.registry import ToolRegistry, ToolConfig
        config = ToolConfig()
        registry = ToolRegistry(config)
        tools = registry.get_enabled_tools()
        print(f'✅ Successfully loaded {len(tools)} tools')
        assert len(tools) > 20, 'Expected at least 20 tools'
        "

  build-docs:
    runs-on: ubuntu-latest
    needs: [test, lint, sanity-check]
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: read
      pages: write
      id-token: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Generate test coverage report
      run: |
        python -m pytest \
          -m "not integration and not real_params and not performance" \
          --cov=tools \
          --cov-report=html \
          --cov-report=xml \
          --tb=short \
          --disable-warnings \
          -n auto

    - name: Setup Pages
      id: pages
      uses: actions/configure-pages@v4
      with:
        enablement: true
      continue-on-error: true

    - name: Upload coverage report artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./htmlcov
      if: steps.pages.outcome == 'success'

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
      if: steps.pages.outcome == 'success'

    - name: Pages deployment status
      run: |
        if [ "${{ steps.pages.outcome }}" == "success" ]; then
          echo "✅ Coverage report deployed to GitHub Pages"
          echo "📊 View coverage at: ${{ steps.deployment.outputs.page_url }}"
        else
          echo "⚠️  GitHub Pages not enabled or configured"
          echo "💡 To enable Pages:"
          echo "   1. Go to Settings → Pages"
          echo "   2. Set Source to 'GitHub Actions'"
          echo "   3. Save settings"
          echo "📁 Coverage report generated locally in ./htmlcov/"
        fi

  # Real parameter tests (only on main branch with secrets)
  # Note: GROQ_API_KEY and XAI_API_KEY secrets need to be configured in repository settings
  real-params-test:
    runs-on: ubuntu-latest
    needs: [test, lint, sanity-check]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
      GITHUB_VERIFY_SSL: "true"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run real parameter tests
      run: |
        python -m pytest \
          -m "real_params" \
          --tb=short \
          -v \
          --disable-warnings \
          -s \
          --maxfail=3
      continue-on-error: true
