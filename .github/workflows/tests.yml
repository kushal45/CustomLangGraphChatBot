name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  # Test environment variables for GitHub Actions
  GITHUB_VERIFY_SSL: "false"
  XAI_API_KEY: "test-xai-key"
  GROQ_API_KEY: "test-groq-key"
  SLACK_WEBHOOK_URL: "https://hooks.slack.com/test"
  EMAIL_SMTP_SERVER: "smtp.test.com"
  EMAIL_USERNAME: "test@example.com"
  EMAIL_PASSWORD: "test-password"
  JIRA_URL: "https://test.atlassian.net"
  JIRA_USERNAME: "test@example.com"
  JIRA_API_TOKEN: "test-jira-token"

jobs:
  # Fast-fail job to catch obvious issues quickly
  quick-check:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install minimal dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest

    - name: Quick syntax and import check
      run: |
        python -m py_compile tools/*.py
        python -c "from tools.registry import ToolRegistry; print('‚úÖ Registry imports OK')"
        python -c "from api import app; print('‚úÖ API imports OK')"
        python -c "from workflow import create_review_workflow; print('‚úÖ Workflow imports OK')"

    - name: Quick test run (fastest tests only)
      run: |
        python -m pytest tests/unit/test_registry.py -v --tb=short --disable-warnings

  test:
    needs: quick-check
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Overall job timeout to prevent hanging
    strategy:
      fail-fast: false  # Continue running other jobs even if one fails
      matrix:
        python-version: ["3.12"]
        test-category: [unit, integration, performance]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/requirements-test.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y git

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        # Verify pytest-timeout is available
        python -c "import pytest_timeout; print('pytest-timeout installed successfully')"

    - name: Set up test environment
      run: |
        # Set GitHub token for real API tests (if available)
        if [ -n "${{ secrets.GITHUB_TOKEN }}" ]; then
          echo "GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}" >> $GITHUB_ENV
        else
          echo "GITHUB_TOKEN=test-github-token" >> $GITHUB_ENV
        fi
    
    - name: Run unit tests (all individual component tests)
      if: matrix.test-category == 'unit'
      timeout-minutes: 25
      run: |
        python -m pytest \
          tests/unit/ \
          -m "not integration and not real_params and not performance" \
          --tb=short \
          -v \
          --disable-warnings \
          --cov=tools \
          --cov=scripts \
          --cov=nodes \
          --cov-report=xml \
          --cov-report=term-missing \
          --maxfail=15 \
          -n 2 \
          --durations=10

    - name: Run integration tests (component interaction and workflow tests)
      if: matrix.test-category == 'integration'
      timeout-minutes: 30
      run: |
        python -m pytest \
          tests/integration/ \
          -m "not real_params and not performance" \
          --tb=short \
          -v \
          --disable-warnings \
          --cov=tools \
          --cov=workflow \
          --cov=nodes \
          --cov=api \
          --cov-report=xml \
          --cov-report=term-missing \
          --maxfail=10 \
          -n 2 \
          --durations=15

    - name: Run performance tests (load and stress testing)
      if: matrix.test-category == 'performance'
      timeout-minutes: 15
      run: |
        python -m pytest \
          tests/performance/ \
          -m "performance or not performance" \
          --tb=short \
          -v \
          --disable-warnings \
          --durations=20 \
          --maxfail=5 \
          --benchmark-only \
          --benchmark-sort=mean

    - name: Test debugging scripts functionality
      if: matrix.test-category == 'unit'
      timeout-minutes: 15
      run: |
        # Test node debugging script
        python scripts/debug_node.py --node start_review_node --sample-state --output-file test_state.json

        # Test state inspection script
        python scripts/inspect_state.py --file test_state.json --analyze

        # Test node serialization system
        python scripts/node_serialization.py

        # Test node replay system
        python scripts/node_replay.py

        # Test node profiling system
        python scripts/node_profiling.py

        # Test flow visualization system
        python scripts/node_flow_diagrams.py

        echo "‚úÖ All debugging scripts executed successfully"

    - name: Test workflow debugging capabilities
      if: matrix.test-category == 'integration'
      timeout-minutes: 10
      run: |
        # Test workflow state transitions
        python -c "
        import asyncio
        from tests.integration.test_workflow_debugging import WorkflowTestFixtures
        from nodes import start_review_node, analyze_code_node, generate_report_node

        async def test_workflow():
            state = WorkflowTestFixtures.create_initial_state()
            result1 = await start_review_node(state)
            state.update(result1)
            result2 = await analyze_code_node(state)
            state.update(result2)
            result3 = await generate_report_node(state)
            state.update(result3)
            print('‚úÖ Workflow execution test passed')

        asyncio.run(test_workflow())
        "

        echo "‚úÖ Workflow debugging capabilities verified"

    - name: Upload coverage to Codecov
      if: matrix.test-category == 'unit' || matrix.test-category == 'integration'
      continue-on-error: true
      timeout-minutes: 3
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: ${{ matrix.test-category }}
        name: codecov-${{ matrix.python-version }}-${{ matrix.test-category }}
        fail_ci_if_error: false

  # Sanity check job for critical functionality
  sanity-check:
    runs-on: ubuntu-latest
    needs: quick-check
    strategy:
      matrix:
        sanity-test: [
          "test_filesystem_tools_sanity",
          "test_analysis_tools_sanity",
          "test_ai_analysis_tools_sanity",
          "test_github_tools_sanity",
          "test_communication_tools_sanity",
          "test_debugging_tools_sanity",
          "test_workflow_integration_sanity"
        ]
      fail-fast: false  # Continue running other tests even if one fails

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run sanity check - ${{ matrix.sanity-test }}
      timeout-minutes: 15
      run: |
        python -m pytest tests/unit/test_module_sanity.py::${{ matrix.sanity-test }} \
          --tb=short -v --disable-warnings -s
      continue-on-error: true  # Allow individual sanity tests to fail without stopping the workflow

  lint:
    runs-on: ubuntu-latest
    needs: quick-check
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run black (code formatting)
      run: |
        black --check --diff tools/ tests/ || true

    - name: Run isort (import sorting)
      run: |
        isort --check-only --diff tools/ tests/ || true

    - name: Run flake8 (style guide)
      run: |
        flake8 tools/ tests/ --max-line-length=100 --extend-ignore=E203,W503,E501 || true

    - name: Run pylint (static analysis)
      run: |
        pylint tools/ --disable=C0114,C0115,C0116,R0903,R0913,W0613 --max-line-length=100 || true

  security:
    runs-on: ubuntu-latest
    needs: quick-check
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run bandit (security analysis)
      run: |
        bandit -r tools/ -f json -o bandit-report.json || true
        bandit -r tools/ || true

    - name: Run safety (dependency security)
      run: |
        safety check --json --output safety-report.json || true
        safety check || true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  integration-test:
    runs-on: ubuntu-latest
    needs: [test, lint, sanity-check]
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run segregated test suite
      run: |
        python -m pytest \
          -m "not integration and not real_params and not performance" \
          --tb=short \
          -v \
          --disable-warnings \
          -n auto

    - name: Validate setup
      run: |
        python validate_setup.py

    - name: Test tool registry
      run: |
        python -c "
        from tools.registry import ToolRegistry, ToolConfig
        config = ToolConfig()
        registry = ToolRegistry(config)
        tools = registry.get_enabled_tools()
        print(f'‚úÖ Successfully loaded {len(tools)} tools')
        assert len(tools) > 20, 'Expected at least 20 tools'
        "

  build-docs:
    runs-on: ubuntu-latest
    needs: [test, lint, sanity-check]
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: read
      pages: write
      id-token: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Generate test coverage report
      run: |
        python -m pytest \
          -m "not integration and not real_params and not performance" \
          --cov=tools \
          --cov-report=html \
          --cov-report=xml \
          --tb=short \
          --disable-warnings \
          -n auto

    - name: Setup Pages
      id: pages
      uses: actions/configure-pages@v4
      with:
        enablement: true
      continue-on-error: true

    - name: Upload coverage report artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./htmlcov
      if: steps.pages.outcome == 'success'

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
      if: steps.pages.outcome == 'success'

    - name: Pages deployment status
      run: |
        if [ "${{ steps.pages.outcome }}" == "success" ]; then
          echo "‚úÖ Coverage report deployed to GitHub Pages"
          echo "üìä View coverage at: ${{ steps.deployment.outputs.page_url }}"
        else
          echo "‚ö†Ô∏è  GitHub Pages not enabled or configured"
          echo "üí° To enable Pages:"
          echo "   1. Go to Settings ‚Üí Pages"
          echo "   2. Set Source to 'GitHub Actions'"
          echo "   3. Save settings"
          echo "üìÅ Coverage report generated locally in ./htmlcov/"
        fi

  # Real parameter tests (only on main branch with secrets)
  # Note: GROQ_API_KEY and XAI_API_KEY secrets need to be configured in repository settings
  real-params-test:
    runs-on: ubuntu-latest
    needs: [test, lint, sanity-check]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
      GITHUB_VERIFY_SSL: "true"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run real parameter tests
      run: |
        python -m pytest \
          -m "real_params" \
          --tb=short \
          -v \
          --disable-warnings \
          -s \
          --maxfail=3
      continue-on-error: true
